+-----------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+--------------------------------+----+
|title                                                                                                                  |author                                                                                                              |journal/conference              |year|
+-----------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+--------------------------------+----+
|Spatial self-attention network with self-attention distillation for fine-grained image recognition.                    |[Adu Asare Baffour, Zhen Qin 0002, Yong Wang, Zhiguang Qin, Kim-Kwang Raymond Choo]                                 |J. Vis. Commun. Image Represent.|2021|
|Synthesizer: Rethinking Self-Attention for Transformer Models.                                                         |[Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, Che Zheng]                                            |ICML                            |2021|
|Shatter: An Efficient Transformer Encoder with Single-Headed Self-Attention and Relative Sequence Partitioning.        |[Ran Tian, Joshua Maynez, Ankur P. Parikh]                                                                          |CoRR                            |2021|
|Multi-View Self-Attention Based Transformer for Speaker Recognition.                                                   |[Rui Wang, Junyi Ao, Long Zhou, Shujie Liu 0001, Zhihua Wei, Tom Ko, Qing Li, Yu Zhang]                             |CoRR                            |2021|
|Multi-Head Self-Attention via Vision Transformer for Zero-Shot Learning.                                               |[Faisal Alamri, Anjan Dutta 0001]                                                                                   |CoRR                            |2021|
|A Vision Transformer with Improved LeFF and Vision Combinative Self-attention Mechanism for Waste Image Classification.|[Yuxiang Guo, Di Cao, Wuchao Li, Shang Hu, Jiabin Gao, Lixin Huang, Zengrong Ye]                                    |ICISCAE (IEEE)                  |2021|
|COViT-GAN: Vision Transformer forCOVID-19 Detection in CT Scan Imageswith Self-Attention GAN forDataAugmentation.      |[Ara Abigail E. Ambita, Eujene Nikka V. Boquio, Prospero C. Naval]                                                  |ICANN (2)                       |2021|
|E.T.: re-thinking self-attention for transformer models on GPUs.                                                       |[Shiyang Chen, Shaoyi Huang, Santosh Pandey, Bingbing Li, Guang R. Gao, Long Zheng 0001, Caiwen Ding, Hang Liu 0001]|SC                              |2021|
|Dual Aspect Self-Attention based on Transformer for Remaining Useful Life Prediction.                                  |[Zhizheng Zhang 0008, Wen Song, Qiqiang Li]                                                                         |CoRR                            |2021|
|EEG-Transformer: Self-attention from Transformer Architecture for Decoding EEG of Imagined Speech.                     |[Young Eun Lee, Seo-Hyun Lee]                                                                                       |CoRR                            |2021|
+-----------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+--------------------------------+----+